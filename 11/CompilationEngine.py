########################################################################################################################
#                                               CompilationEngine                                                      #
# programmer : Snir Sharristh                                                                                          #
#                                                                                                                      #
# Effects the actual compilation output. Gets its input from a JackTokenizer and emits its parsed structure into an    #
# Ouput file /stream. the output is generated by a series of compilexxx() routines, one for every syntactic element    #
# xxx of the Jack grammer. The contract between these routines is that each compilexxx() routine should read the       #
# syntactic construct xxx from the input, advance() the tokenizer exactly beyond xxx, and output the parsing of xxx.   #
# Thus, compilexxx() may only be called if indeed xxx is the next syntactic element of the input.                      #
#                                                                                                                      #
########################################################################################################################
from JackTokenizer import *
from SymbolTable import SymbolTable
from VMWriter import VMWriter

TYPES = {'KEYWORD': 'keyword', 'SYMBOL': 'symbol', 'IDENTIFIER': 'identifier', 'INT_CONST': 'integerConstant',
         'STRING_CONST': 'stringConstant', 'INVALID_TOKEN_TYPE': 'INVALID_TOKEN_TYPE'}
KINDS_DICT = {'argument': 'argument', 'static': 'static', 'var': 'local', 'field': 'field'}

OPS = ['+', '-', '*', '/', '&', '|', '<', '>', '=']
UNARY_OP = ['-', '~']
FUNC_NAME_FORMAT = '{}.{}'


class CompilationEngine:
    def __init__(self, input_path, output_path):
        """
        creates a new compilation engine with the given input and output. the next routine called must be compileClass()
        :param input_path: input stream/file
        :param output_path: output stream/file
        """
        self._root = None
        self._current_node = None
        self.tokenizer = JackTokenizer(input_path)
        self._class_name = ''
        self._if_ct = 0
        self._while_ct = 0
        self._symbols = SymbolTable()
        self._writer = VMWriter(output_path)
        self.CompileClass()

    def CompileClass(self):
        """
        Compiles a complete class.
        """
        self.tokenizer.advance()  # skip the keyword class
        self.tokenizer.advance()
        self._class_name = self.tokenizer.identifier()
        self.tokenizer.advance()  # skip '{'
        self.CompileClassVarDec()
        self.CompileSubroutine()
        self._writer.close()

    def CompileClassVarDec(self):
        """
        Compiles a static declaration or a field declaration.
        """
        peek = self.tokenizer.peek()
        while 'static' in peek or 'field' in peek:
            self.tokenizer.advance()
            kind = self.tokenizer.keyWord()  # field/static
            self.tokenizer.advance()
            type = self.tokenizer.keyWord()  # type
            self.tokenizer.advance()
            name = self.tokenizer.identifier()  # name
            self.tokenizer.advance()
            self._symbols.Define(name, type, kind)
            while self.tokenizer.symbol() == ',':
                self.tokenizer.advance()  # skip ','
                self._symbols.Define(self.tokenizer.identifier(), type, kind)  # name
                self.tokenizer.advance()
            peek = self.tokenizer.peek()  # ? ';'

    def CompileSubroutine(self):
        """
        Compiles a complete method, function, or constructor.
        """
        peek = self.tokenizer.peek()
        while 'function' in peek or 'constructor' in peek or 'method' in peek:
            self._symbols.startSubroutine()
            self.tokenizer.advance()
            kind = self.tokenizer.keyWord()  # const/func/method
            if kind == 'constructor':
                self._writer.push('constant', self._symbols.field_ct)
                self._writer.write_call('Memory.alloc', 1)
                self._writer.pop('pointer', 0)
            elif kind == 'method':
                self._writer.push('argument', 0)
                self._writer.pop('pointer', 0)
            self.tokenizer.advance()
            self.tokenizer.advance()
            func_name = self.tokenizer.identifier()  # name
            self.tokenizer.advance()  # skip '('
            self.CompileParameterList()
            arg_ct = self._symbols.arg_ct
            name = FUNC_NAME_FORMAT.format(self._class_name, func_name)
            self._writer.write_function(name, arg_ct)
            self.tokenizer.advance()  # skip ')'
            self.tokenizer.advance()  # skip '{'
            peek = self.tokenizer.peek()
            if 'var' in peek:
                self.CompileVarDec()
            self.CompileStatements()
            self.tokenizer.advance()  # skip '}'

    def CompileParameterList(self):
        """
        Compiles a (possibly empty) parameter list, not including the enclosing ()
        """
        # param_list = ET.SubElement(self._current_node, 'parameterList')
        kind = 'argument'
        peek = self.tokenizer.peek()
        if peek != ')':
            self.tokenizer.advance()
            type = self.tokenizer.keyWord()  # type
            self.tokenizer.advance()
            name = self.tokenizer.identifier()  # name
            peek = self.tokenizer.peek()
            self._symbols.Define(name, type, kind)
        while peek == ',':
            self.tokenizer.advance()
            self.tokenizer.advance()  # skip ','
            type = self.tokenizer.keyWord()  # type
            self.tokenizer.advance()
            name = self.tokenizer.identifier()  # name
            peek = self.tokenizer.peek()
            self._symbols.Define(name, type, kind)

    def CompileVarDec(self):
        """
        Compiles a var declaration.
        """
        peek = self.tokenizer.peek()
        while 'var' in peek:
            self.tokenizer.advance()
            kind = self.tokenizer.keyWord()
            self.tokenizer.advance()
            type = self.tokenizer.keyWord()
            self.tokenizer.advance()
            name = self.tokenizer.identifier()
            self.tokenizer.advance()
            self._symbols.Define(name, type, kind)
            while self.tokenizer.symbol() == ',':
                self.tokenizer.advance()  # skip ','
                name = self.tokenizer.identifier()  # name
                self.tokenizer.advance()
                self._symbols.Define(name, type, kind)
            peek = self.tokenizer.peek()

    def CompileStatements(self):  # TODO: add VM to each ones
        """
        Compiles a sequence of statements, not including the enclosing "{}"
        """
        peek = self.tokenizer.peek()
        while 'let' in peek or 'if' in peek or 'while' in peek or 'do' in peek or 'return' in peek:
            if 'let' in peek:
                self.CompileLet()
            elif 'if' in peek:
                self.CompileIf()
            elif 'while' in peek:
                self.CompileWhile()
            elif 'do' in peek:
                self.CompileDo()
            elif 'return' in peek:
                self.CompileReturn()
            peek = self.tokenizer.peek()

    def CompileDo(self):
        """
        Compiles a do statement.
        """
        self.tokenizer.advance()  # skip 'do' statement
        self.tokenizer.advance()
        name = self.tokenizer.identifier()  # name of what to call
        peek = self.tokenizer.peek()
        while peek == '.':
            self.tokenizer.advance()  # skip '.'
            name += '.'
            self.tokenizer.advance()
            name += self.tokenizer.identifier()
            peek = self.tokenizer.peek()
        self.tokenizer.advance()  # skip '('
        self.CompileExpressionList()
        self.tokenizer.advance()  # skip ')'
        self._writer.pop('temp', 0)
        self.tokenizer.advance()  # skip ';

    def CompileLet(self):
        """
        Compiles a let statement.
        """
        self.tokenizer.advance()  # skip 'let'
        self.tokenizer.advance()
        name = self.tokenizer.identifier()
        kind = self._symbols.KindOf(name)
        index = self._symbols.IndexOf(name)
        peek = self.tokenizer.peek()
        if peek == '[':
            self.tokenizer.advance()  # skip '['
            self.tokenizer.advance()
            self.CompileExpression()
            self.tokenizer.advance()  # skip ']'
            self._writer.push(KINDS_DICT[kind], index)
            self._writer.write_cmd('add')
            self._writer.pop('temp', 0)
            self.tokenizer.advance()  # skip '='
            self.tokenizer.advance()
            self.CompileExpression()
            self.tokenizer.advance()  # skip ';'
            self._writer.push('temp', 0)
            self._writer.pop('pointer', 1)
            self._writer.pop('that', 0)
        else:
            self.tokenizer.advance()  # skip '='
            self.CompileExpression()
            self.tokenizer.advance()  # skip ';'
            self._writer.pop(KINDS_DICT[kind], index)

    def CompileWhile(self):
        """
        Compiles a while statement.
        """
        self._while_ct += 1
        start_while = 'while_true{}'.format(self._while_ct)
        end_while = 'while_end{}'.format(self._while_ct)
        self.tokenizer.advance()  # skip while
        self.tokenizer.advance()
        self.tokenizer.advance()  # skip '('
        self._writer.write_label(start_while)
        self.CompileExpression()
        self._writer.write_cmd('not')
        self._writer.write_if(end_while)
        self.tokenizer.advance()
        self.tokenizer.advance()  # skip '){'
        self.CompileStatements()
        self._writer.write_goto(start_while)
        self._writer.write_goto(end_while)
        self.tokenizer.advance()  # skip '}'

    def CompileReturn(self):
        """
        Compiles a return statement.
        """
        self.tokenizer.advance()  # skip 'return'
        peek = self.tokenizer.peek()
        if peek != ';':
            self.tokenizer.advance()
            self.CompileExpression()
            self.tokenizer.advance()
        else:
            self.tokenizer.advance()
            self._writer.push('constant', 0)
        self._writer.write_return()

    def CompileIf(self):
        """
        Compiles an if statement, possibly with a trailing else clause.
        """
        self._if_ct += 1
        if_t = 'if_true' + str(self._if_ct)
        if_f = 'if_false' + str(self._if_ct)
        end_if = 'if_end' + str(self._if_ct)
        self.tokenizer.advance()
        self.tokenizer.advance()  # skip if
        self.tokenizer.advance()  # skip '('
        self.CompileExpression()
        self.tokenizer.advance()  # skip ')'
        self._writer.write_if(if_t)
        self._writer.write_goto(if_f)
        self._writer.write_label(if_t)
        self.tokenizer.advance()  # skip '{'
        self.CompileStatements()
        self.tokenizer.advance()  # skip '}'
        peek = self.tokenizer.peek()
        if peek == 'else':
            self.tokenizer.advance()
            self._writer.write_goto(end_if)
            self._writer.write_label(if_f)
            self.tokenizer.advance()  # skip 'else{'
            self.CompileStatements()
            self._writer.write_label(end_if)
            self.tokenizer.advance()  # skip '}'
        else:
            self._writer.write_label(if_f)

    def CompileExpression(self):
        """
        Compiles an expression.
        """
        self.CompileTerm()
        peek = self.tokenizer.peek()
        while peek in OPS:
            self.tokenizer.advance()
            op = self.tokenizer.current_token
            if op == '*':
                self._writer.write_call('Math.multiply', 2)
            elif op == '/':
                self._writer.write_call('Math.divide', 2)
            else:
                self._writer.write_cmd(op)
            self.tokenizer.advance()  # skip 'op'
            self.CompileTerm()
            peek = self.tokenizer.peek()

    def CompileTerm(self):
        """
        Compiles a term. This routine is faced with a slight difficulty when trying to decide between some of the
        alternative parsing rules. Specifically, if the current token is an identifier, the routine must distinguish
        between a variable, an array entry, and a subroutine call. A single look-ahead token, which may be one
        of [, (, or . suffices to distinguish between the three possibilities. Any other token is not
        part of this term and should not be advanced over.
        """
        if self.tokenizer.current_token in UNARY_OP:
            op = self.tokenizer.current_token
            self.tokenizer.advance()
            self.CompileTerm()
            self._writer.write_cmd(op)
        elif '(' in self.tokenizer.current_token:
            self.tokenizer.advance()  # skip '('
            self.CompileExpression()
            self.tokenizer.advance()  # skip ')'
        elif self.tokenizer.tokenType() == 'INT_CONST':
            self._writer.push('constant', self.tokenizer.current_token)
            self.tokenizer.advance()
        elif self.tokenizer.tokenType() == 'STRING_CONST':
            self._WriteStringConst()
        elif self.tokenizer.tokenType() == 'KEYWORD':
            self._WriteKeyword()
        else:  # if we got here thus we have a identifier (might be array)
            self._WriteIdentifier()

    def CompileExpressionList(self):
        """
        Compiles a (possibly empty) comma-separated list of expressions.
        """
        peek = self.tokenizer.peek()
        counter = 0
        while peek != ')':
            counter += 1
            self.tokenizer.advance()
            if peek == ',':
                self.tokenizer.advance()
            self.CompileExpression()
            peek = self.tokenizer.peek()
        return counter

    def _WriteStringConst(self):
        string_const = self.tokenizer.stringVal()
        self._writer.push('constant', len(string_const))
        self._writer.write_call('String.new', 1)
        for c in string_const:
            self._writer.push('constant', ord(c))
            self._writer.write_call('String.appendChar', 2)

    def _WriteKeyword(self):
        if self.tokenizer.current_token == 'this':
            self._writer.push('pointer', 0)
        else:
            self._writer.push('constant', 0)
            if self._current_node == 'true':
                self._writer.write_cmd('neg')

    def _WriteIdentifier(self):
        self.tokenizer.advance()
        peek = self.tokenizer.peek()
        if peek == '[':
            name = self.tokenizer.current_token
            self.tokenizer.advance()
            self.tokenizer.advance()  # skip '['
            self.CompileExpression()
            self.tokenizer.advance()  # skip ']'
            kind = self._symbols.KindOf(name)
            index = self._symbols.IndexOf(name)
            self._writer.push(KINDS_DICT[kind], index)
            self._writer.write_cmd('add')
            self._writer.pop('pointer', 1)
            self._writer.push('that', 0)
        elif peek == '.' or peek == '(':
            name = self.tokenizer.current_token
            args = 0
            if peek == '.':
                self.tokenizer.advance()  # skip '.'
                self.tokenizer.advance()
                name = FUNC_NAME_FORMAT.format(name, self.tokenizer.current_token)
                type = self._symbols.TypeOf(name)
                if type:
                    kind = self._symbols.KindOf(name)
                    index = self._symbols.IndexOf(name)
                    self._writer.push(KINDS_DICT[kind], index)
                    args += 1
            else:
                self.tokenizer.advance()  # skip '('
                self.tokenizer.advance()
                name = FUNC_NAME_FORMAT.format(name, self.tokenizer.current_token)
                self._writer.push('pointer', 0)
                args += 1
            self.tokenizer.advance()
            args += self.CompileExpressionList()
            self._writer.write_call(name, args)
            self.tokenizer.advance()
        else:
            name = self.tokenizer.current_token
            kind = self._symbols.KindOf(name)
            index = self._symbols.IndexOf(name)
            self._writer.push(KINDS_DICT[kind], index)
